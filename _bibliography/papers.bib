---
---
@misc{handina2024understandingmodelselectionlearning,
      bibtex_show={true},
      selected={true},
      title={Understanding Model Selection For Learning In Strategic Environments}, 
      author={Tinashe Handina and Eric Mazumdar},
      year={2024},
      eprint={2402.07588},
      archivePrefix={arXiv},
      primaryClass={cs.GT},
      url={https://arxiv.org/abs/2402.07588}, 
}

@InProceedings{pmlr-v178-christianson22a,
  title = 	 {Chasing Convex Bodies and Functions with Black-Box Advice},
  author =       {Christianson, Nicolas and Handina, Tinashe and Wierman, Adam},
  booktitle = 	 {Proceedings of Thirty Fifth Conference on Learning Theory},
  pages = 	 {867--908},
  year = 	 {2022},
  editor = 	 {Loh, Po-Ling and Raginsky, Maxim},
  volume = 	 {178},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--05 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v178/christianson22a/christianson22a.pdf},
  url = 	 {https://proceedings.mlr.press/v178/christianson22a.html},
  abstract = 	 {We consider the problem of convex function chasing with black-box advice, where an online decision-maker aims to minimize the total cost of making and switching between decisions in a normed vector space, aided by black-box advice such as the decisions of a machine-learned algorithm. The decision-maker seeks cost comparable to the advice when it performs well, known as \emph{consistency}, while also ensuring worst-case \emph{robustness} even when the advice is adversarial. We first consider the common paradigm of algorithms that switch between the decisions of the advice and a competitive algorithm, showing that no algorithm in this class can improve upon 3-consistency while staying robust. We then propose two novel algorithms that bypass this limitation by exploiting the problemâ€™s convexity. The first, $\textsc{Interp}$, achieves $(\sqrt{2}+\epsilon)$-consistency and $\mathcal{O}(\frac{C}{\epsilon^2})$-robustness for any $\epsilon &gt; 0$, where $C$ is the competitive ratio of an algorithm for convex function chasing or a subclass thereof. The second, $\textsc{BdInterp}$, achieves $(1+\epsilon)$-consistency and $\mathcal{O}(\frac{CD}{\epsilon})$-robustness when the problem has bounded diameter $D$. Further, we show that $\textsc{BdInterp}$ achieves near-optimal consistency-robustness trade-off for the special case where cost functions are $\alpha$-polyhedral.}
}

@inproceedings{Sehwag2021RobustLM,
  title={Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?},
  author={Vikash Sehwag and Saeed Mahloujifar and Tinashe Handina and Sihui Dai and Chong Xiang and Mung Chiang and Prateek Mittal},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:237940634}
}
